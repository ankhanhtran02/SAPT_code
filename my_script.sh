python3 src/run_t5.py \
  --do_train \
  --do_predict \
  --predict_with_generate \
  --model_name_or_path google-t5/t5-small \
  --data_dir CODETASK_Benchmark \
  --task_order CodeTrans,CodeSearchNet,BFP,CONCODE \
  --gen_data_dir "generated_data/lora_gen_long_t5" \
  --task_config_dir configs/CodeTask/BFP \
  --output_dir "logs_and_outputs/your_job_name/outputs/5-bfp" \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 32 \
  --gradient_accumulation_steps 2 \
  --learning_rate 0.0003 \
  --num_train_epochs 1 \
  # --bf16 \
  --run_name "your_job_name" \
  --max_source_length 512 \
  --max_target_length 50 \
  --generation_max_length 50 \
  --add_task_name False \
  --add_dataset_name False \
  --overwrite_output_dir \
  --overwrite_cache \
  --lr_scheduler_type "constant" \
  --warmup_steps 0 \
  --logging_strategy "steps" \
  --logging_steps 10 \
  --metric_for_best_model eval_exact_match_for_imdb \
  --evaluation_strategy "steps" \
  --save_strategy "steps" \
  --save_total_limit 1 \
  --load_best_model_at_end \
  --lora_r 8 \
  --lora_alpha 32 \
  --lora_dropout 0.0 \
  --data_replay_freq -1 \
  --kl_ratio 0.1 \
  --attn_temperature 1
